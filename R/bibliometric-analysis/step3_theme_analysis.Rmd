---
title: "Theme analysis for camera trap publications"
author: "Ada Y. Sánchez-Mercado and José R. Ferrer Paris"
date: 2020-04-03
output: word_document
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                           eval = TRUE,
                 warning=FALSE,
                 fig.pos='H'
                )
```

 Load required packages

```{r packages}
library(bibliometrix) #the library for bibliometrics
require(topicmodels) #for topic modeling
library(quanteda) #a library for quantitative text analysis
require(ggplot2) #visualization
library(dplyr) #for data munging
library("RColorBrewer") # user friendly color palettes
library(tidytext)
library("ldatuning")
```

Load data from previous step:

```{r load data}
load(file=sprintf("%s/ISI-camera-corpus.rda",Rdata.dir))
load(file=sprintf("%s/%s.rda",Rdata.dir,data.set.id))
```
This analysis is based on the data-set created from the search with ID: `r data.set.id`.

# Standard bibliometric analysis

This would output several summaries for the dataset, de-activating the output for now.

```{r biblioAnalysis, eval=FALSE, echo=FALSE}
results_M <- biblioAnalysis(ISI.search.df, sep = ";")
summary(object=results_M,k=20,pause=FALSE)
```

# Document Term Matrix

Create DTM (Document Term Matrix). Common format for text analysis. A DTM is a matrix in which rows are documents, columns are terms, and cells indicate how often each term occurred in each document.

```{r basic dfm}
ISI.camera.dfm <- dfm(ISI.camera.bigram, thesaurus = camera_thesaurus)
ISI.camera.dfm
```

There are too many features. Lets simplify it by trimming the dfm to include words that have appeared at least 20 times in the corpus.

```{r trimmed dfm}
ISI.camera.dfm <- dfm_trim(ISI.camera.dfm, min_termfreq = 20)
```

Better?

# Feature co-occurrence matrix

```{r fcm full}
ISI.camera.fcm <- fcm(ISI.camera.dfm)
```
Extract top 50 keywords based on abstracts and create a feature co-occurrence matrix based on the top 50.

```{r fcm top50}
topfeatures(ISI.camera.fcm, 50)
feat <- names(topfeatures(ISI.camera.fcm, 50))
ISI.camera.fcm <- fcm_select(ISI.camera.fcm, feat)
```


We could use this to plot the network, but this step takes time, we are skipping this for now.
```{r fcm plot,eval=FALSE}
size <- log(colSums(dfm_select(ISI.camera.dfm, feat)))

textplot_network(ISI.camera.fcm, min_freq = 0.5, vertex_size = size/max(size) * 3)
```

Plot top keywords frequencies instead:

```{r kwd plot}
freq <- textstat_frequency(ISI.camera.dfm, n = 50)
ISI.camera.dfm %>%
textstat_frequency(n = 20) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
```

Also plot a word cloud (but note that some words are excluded due to their size).

```{r wordcloud}

textplot_wordcloud(ISI.camera.dfm, max_words = 100,
  random.order=FALSE, rot.per=0.35,
  colors=brewer.pal(8, "Dark2"))
```

# LDA model
Now,apply Natural Language Processing and Topic Modeling to abstracts to identify the topics published in camera trap research.

We transform the DFM to DTM

```{r dtm}
ISI.camera.dtm <- convert(ISI.camera.dfm, to = "topicmodels")
```

Now we need determine what is the optimal number of topics we should specify in the LDA model

Package ldatuning realizes 4 metrics to select perfect number of topics for LDA model.

```{r optimal topic number}
if (!exists("result")) {
  result <- FindTopicsNumber(
    ISI.camera.dtm,
    topics = c(5:15,seq(from = 16, to = 61, by = 5)),
    metrics = c("CaoJuan2009"),
    method = "Gibbs",
    control = list(seed = 77),
    mc.cores = 2L,
    verbose = TRUE
  )
}

plot(CaoJuan2009~topics,result)

```

We will set the number of terms to 14

Now, we can fit our first LDA model

```{r lda}
ISI.camera.lda <- LDA(ISI.camera.dtm, control=list(seed=0), k = 14)
```

Show top 10 words pertaining to each topic

```{r ldaterms}
terms(ISI.camera.lda, 10)
```

Obtain the most likely topics for each document, and show topic allocation for the first documents

```{r topic allocation}
tt <- topics(ISI.camera.lda)
docvars(ISI.camera.dfm, 'topic') <- tt[match(row.names(ISI.camera.dfm),names(tt))]


head(topics(ISI.camera.lda), 5)
```


lists the document to (primary) topic assignments:
```{r primary topic}
prd.topic <- topics(ISI.camera.lda)
table(prd.topic)
```

The *tidytext* package provides this method for extracting the per-topic-per-word probabilities, called  β(“beta”), from the model.

```{r beta}
ap_topics <- tidy(ISI.camera.lda, matrix = "beta")
ap_topics
```

Now we can build the tidy data frame for the keywords. For this one, we need to use unnest() from tidyr, because they are in a list-column.


```{r beta per topic}
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
```

Visualization

```{r beta_topic plot}
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
```

```{r top terms}
as.data.frame(ap_top_terms)
```

Dendogram to evaluate how similar are the topics

```{r dendogram}
#str(ISI.camera.lda)
my.model <- ISI.camera.lda@beta
distance_matrix <- dist(my.model, method="euclidean")
plot(hclust(distance_matrix), cex = 1)

```

Document-topic probabilities: Besides estimating each topic as a mixture of words, LDA also models each document as a mixture of topics.

We can examine the per-document-per-topic probabilities, called
γ (“gamma”), with the matrix = "gamma" argument to tidy()

```{r lda gamma}
lda_gamma <- tidy(ISI.camera.lda, matrix = "gamma")
lda_gamma
```
How are the probabilities distributed? Let’s visualize them

```{r boxplot gamma}
boxplot(lda_gamma$gamma ~ lda_gamma$topic)

```

Hmm, few documents are classified in a given topic
with high γ...seems like documents are asigned randomly
to a given topic

Another way to see this is:


```{r total hist gamma}
ggplot(lda_gamma, aes(gamma)) +
geom_histogram() +
scale_y_log10() +
labs(title = "Distribution of probabilities for all topics",
y = "Number of documents", x = expression(gamma))

```

There are many values near zero, which means there are many documents that do not belong in each topic. Also, there are few values near γ= 1 these are the documents that do belong in those topics. This distribution shows that documents are being not well discriminated as belonging to a topic or not. We can also look at how the probabilities are distributed within each topic



```{r hist gamma}
ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
geom_histogram(show.legend = FALSE) +
facet_wrap(~ topic, ncol = 4) +
scale_y_log10() +
labs(title = "Distribution of probability for each topic",
y = "Number of documents", x = expression(gamma))
```

We can use this information to decide how many topics for our topic modeling procedure. When we tried options higher than 6, the distributions for γ started to look very flat toward  γ= 1; documents were not getting sorted into topics very well.

Convert to a data frame

```{r topic df}
prd.topic <- as.data.frame(as.table(prd.topic))
colnames(prd.topic)<- c("UT", "lda_topic")
str(prd.topic)
```

Now get back to the complete collection of literature review
and combine it


```{r merge}
ISI.topic.df <- merge(ISI.search.df, prd.topic, by = "UT", all.x = T)
M3 <- subset(ISI.topic.df,search.group %in% "cameratrap")
M3$topic <- ifelse(is.na(M3$lda_topic),99,M3$lda_topic)

summary(M3$lda_topic)
dim(M3)
```

# Topic temporal trends

We can do this by:
1* Calculating the total number of articles that have been published on a topic over a particular period. This will provide information on total research effort within a corpus.
2* by investigating changes in topic popularity over that period. This allow us evaluate which topics are hot (i.e., show positive growth) versus cold (negative growth) within a given research community.

Lets see the first approach

```{r temporal trend 1}
(yrange <- range(M3$PY,na.rm=T))
M3$period <- cut(M3$PY,c(1990,2005,2010,2012,2014,2016,2018,2021))

dts <- with(M3,tapply(UT,list(year=period,topic=topic), length))
dts[is.na(dts)] <- 0
 mosaicplot(dts,col=1:15)
```

We can check the standardized residuals of the contingency table:

```{r temporal trend mosaic}

mosaicplot(as.table(dts[,1:14]), shade = TRUE)
```

Or relative frequencies (This graphs is difficult to interpret)

```{r temporal trend 2}

dts <- with(M3,tapply(UT,list(year=PY,topic=topic), length))
dts[is.na(dts)] <- 0

 mtz <- apply(t(apply(dts,1,function(x) x/sum(x))),1,cumsum)

matplot(sort(unique(M3$PY)),t(mtz),type="b",lty=1,cex=1,col=c("black","grey20", "grey50", "black", "grey70", "red","blue", "green", "orange", "pink"))
#legend(2000, 90,sprintf("topic %s",1:14), pch =c (3,15,17,18,19, 3,15,17,18,19),col=c("black","grey20", "grey50", "black", "grey70", "red","blue", "green", "orange", "pink"), cex=1 )
```
